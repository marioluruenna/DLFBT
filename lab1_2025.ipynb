{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUWm0aQb9BC8"
      },
      "source": [
        "---\n",
        "# <font color=\"#CA3532\">Deep Learning Fundamentals and Basic Tools (2025/2026) - Lab Assignment 1</font>\n",
        "---\n",
        "\n",
        "Last updated on 2025-09-12\n",
        "\n",
        "Please report any bugs to luis.lago@uam.es\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ypcZkZYAigr"
      },
      "source": [
        "This first lab includes the following tasks:\n",
        "\n",
        "- An introduction to [Numpy](https://numpy.org/), the main library for scientific computing and matrix manipulation in Python.\n",
        "\n",
        "- The implementation of the linear regression and logistic regression models.\n",
        "\n",
        "- An introduction to [TensorFlow](https://www.tensorflow.org/), an open source library for Machine Learning developed by Google.\n",
        "\n",
        "- An implementation of a feedforward neural network using Numpy and TensorFlow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "areLUgwp4ZhC"
      },
      "source": [
        "### <font color=\"#CA3532\">Clone the lab's github repo</font>\n",
        "\n",
        "The easiest way of getting all the lab material is to clone our github repository:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUhhu4QTzbDn",
        "outputId": "1136a60c-a36a-4ee3-ed75-8ced4a240329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/luisferuam/DLFBT-LAB"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DLFBT-LAB' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FgFUcw19sIG"
      },
      "source": [
        "### <font color=\"#CA3532\">Instructions</font>\n",
        "\n",
        "- The assignment must be done in pairs.\n",
        "\n",
        "- All the exercises are described in this notebook. You must code your solutions in the file ``dlfbt_lab1.py`` provided as part of the material. Just complete the sections in the code marked with ``TO-DO`` comments and upload the file to the [course Moodle page](https://posgrado.uam.es/course/view.php?id=67718) before the due date.\n",
        "\n",
        "- The only file you have to turn in is ``dlfbt_lab1.py``.\n",
        "\n",
        "- No code must be added out of the ``TO-DO`` blocks.\n",
        "\n",
        "- Do not forget to include your names and NIAs at the beginning of the file.\n",
        "\n",
        "- **Due date:** Friday, 2025-09-26, 13:00.\n",
        "\n",
        "- **Evaluation:** The submitted material will be subjected to a set of automated tests and will be evaluated as *pass* or *fail* based on the results of these tests. Students who receive a *pass* will take a practical exam on Monday, September 29th, which will determine their final grade. In this exam you will be asked to do small modifications to **your** code, in order to verify that it is really **yours**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zPXQteNbA9N"
      },
      "source": [
        "### <font color=\"#CA3532\">Import the libraries</font>\n",
        "\n",
        "The following code cells import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qoR3iD7bLKF"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ3mdiCYu8HZ"
      },
      "source": [
        "import sys\n",
        "sys.path.append('DLFBT-LAB')\n",
        "import dlfbt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DRvswhK6nnj"
      },
      "source": [
        "### <font color=\"#CA3532\">Exercise 0. Numpy basics</font>\n",
        "\n",
        "This exercise must not be turned in.\n",
        "\n",
        "Read these [Numpy notes](https://cs231n.github.io/python-numpy-tutorial/#numpy) and solve the following exercises."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thSzcdJr47_C"
      },
      "source": [
        "# (1) Create a numpy array of dimension 4x3 filled with random integers between\n",
        "#     0 and 5. Assign the result to variable A."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fHeQo1L40Aj"
      },
      "source": [
        "# (2) Create a numpy array of dimension 3x2 filled with random integers between\n",
        "#     0 and 5. Assign the result to variable B."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ1_xlLs8qJl"
      },
      "source": [
        "# (3) Multiply (dot product) the two arrays A and B. Assign the result to\n",
        "#     variable C."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XZ_EkDZ8seR"
      },
      "source": [
        "# (4) Add the array B to the 3x2 first positions of array A."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RckWamU-8uog"
      },
      "source": [
        "# (5) Without using any loop, create the numpy array D with the following\n",
        "#     content:\n",
        "#     [[  1   2   3   4   5   6   7   8   9  10]\n",
        "#      [  2   4   6   8  10  12  14  16  18  20]\n",
        "#      [  3   6   9  12  15  18  21  24  27  30]\n",
        "#      [  4   8  12  16  20  24  28  32  36  40]\n",
        "#      [  5  10  15  20  25  30  35  40  45  50]\n",
        "#      [  6  12  18  24  30  36  42  48  54  60]\n",
        "#      [  7  14  21  28  35  42  49  56  63  70]\n",
        "#      [  8  16  24  32  40  48  56  64  72  80]\n",
        "#      [  9  18  27  36  45  54  63  72  81  90]\n",
        "#      [ 10  20  30  40  50  60  70  80  90 100]]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-iyfoco-67K"
      },
      "source": [
        "### <font color=\"#CA3532\">Exercise 1. Linear regression</font>\n",
        "\n",
        "In the file ``dlfbt_lab1.py``, complete the code of the class ``LinearRegressionModel`` by filling all the ``TO-DO`` blocks, so that it implements a Linear Regression model.\n",
        "\n",
        "You can run the following code to test your implementation. We suggest that you write the code incrementally, testing each method according to the following points."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dlfbt_lab1 import LinearRegressionModel"
      ],
      "metadata": {
        "id": "DHHJRKaxkzF8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARXFq3JMauL_"
      },
      "source": [
        "**1. Implement the ``predict`` method.** Then check your implementation by running the following tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nys-vbuj2qse"
      },
      "source": [
        "Test on a single scalar value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_0mwPQy0gfz"
      },
      "source": [
        "linrm = LinearRegressionModel()\n",
        "linrm.w = np.array([[2.0]])\n",
        "linrm.b = np.array([[1.0]])\n",
        "\n",
        "x = np.array([[3.0]])\n",
        "t = np.array([[6.0]])\n",
        "y = linrm.predict(x)\n",
        "\n",
        "assert y.shape == (1, 1)\n",
        "assert y == [[7.]]\n",
        "assert linrm.get_loss(x, t) == 0.5"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umxu5a5J2tGh"
      },
      "source": [
        "Test on 1D data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh2DEGhr0gl3"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear()\n",
        "dg.create_dataset(noise=0.0)\n",
        "\n",
        "linrm = LinearRegressionModel()\n",
        "linrm.w = dg.a\n",
        "linrm.b = dg.b\n",
        "\n",
        "y = linrm.predict(dg.x)\n",
        "\n",
        "tol = 1.e-15\n",
        "assert y.shape == (100, 1)\n",
        "assert np.abs(y - dg.t).max() < tol\n",
        "assert linrm.get_loss(dg.x, dg.t) < tol"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlWjeCuf2v7q"
      },
      "source": [
        "Test on multidimensional data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq2lUNGn2278"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[-5.0, 2.0, -3.0, 2.0])\n",
        "dg.create_dataset(noise=0.0, n=500)\n",
        "\n",
        "linrm = LinearRegressionModel()\n",
        "linrm.w = dg.a\n",
        "linrm.b = dg.b\n",
        "\n",
        "y = linrm.predict(dg.x)\n",
        "\n",
        "tol = 1.e-15\n",
        "assert y.shape == (500, 1)\n",
        "assert np.abs(y - dg.t).max() < tol\n",
        "assert linrm.get_loss(dg.x, dg.t) < tol"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keTS6hTy2FRz"
      },
      "source": [
        "**2. Implement the ``compute_gradients`` method.** Then check your implementation by running the following tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuJz8Fl08iIZ"
      },
      "source": [
        "Test on a single scalar value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzp9SZVm2UF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10103f8-3178-43d3-f9d2-3ab4b1975e73"
      },
      "source": [
        "linrm = LinearRegressionModel()\n",
        "linrm.w = np.array([[2.0]])\n",
        "linrm.b = np.array([[1.0]])\n",
        "\n",
        "db, dw = linrm.compute_gradients(np.array([[3.0]]), np.array([[6.0]]))\n",
        "\n",
        "assert db.shape == (1, 1)\n",
        "assert db == [[1.]]\n",
        "assert dw.shape == (1, 1)\n",
        "assert dw == [[3.]]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0\n",
            "[[3.]]\n",
            "1.0\n",
            "[[1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFpESZnY2UkZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "61d5f828-46c4-47b2-8de5-36b4812068e4"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear()\n",
        "dg.create_dataset(n=1000, seed=17)\n",
        "\n",
        "linrm = LinearRegressionModel()\n",
        "linrm.w = dg.a\n",
        "linrm.b = dg.b\n",
        "\n",
        "db, dw = linrm.compute_gradients(dg.x, dg.t)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert db.shape == (1, 1)\n",
        "assert np.abs(db[0][0] + 0.13987394) < tol\n",
        "assert dw.shape == (1, 1)\n",
        "assert np.abs(dw[0][0] + 0.581568) < tol"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-581.5680044491766\n",
            "[[-581.56800445]]\n",
            "-0.13987393676982426\n",
            "[[-0.13987394]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-670373371.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.13987394\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.581568\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U68E-lVe8j8e"
      },
      "source": [
        "Test on multidimensional data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nE30Nzv2UW6"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[-5.0, 2.0, -3.0, 2.0])\n",
        "dg.create_dataset(n=50000, seed=17)\n",
        "\n",
        "linrm = LinearRegressionModel()\n",
        "linrm.w = dg.a\n",
        "linrm.b = dg.b\n",
        "\n",
        "db, dw = linrm.compute_gradients(dg.x, dg.t)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert db.shape == (1, 1)\n",
        "assert np.abs(db[0][0] + 0.00704326) < tol\n",
        "assert dw.shape == (4, 1)\n",
        "assert np.abs(dw[0][0] + 0.05353578) < tol\n",
        "assert np.abs(dw[1][0] + 0.03276935) < tol\n",
        "assert np.abs(dw[2][0] + 0.00337341) < tol\n",
        "assert np.abs(dw[3][0] + 0.03293776) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8vTAjiR2U3h"
      },
      "source": [
        "**3. Implement the ``gradient_step`` method.** Then check your implementation by running the following tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FOrD1MbXANF"
      },
      "source": [
        "Test on 1D data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F4zH8bqvZVJ"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear1D()\n",
        "dg.create_dataset(seed=17)\n",
        "\n",
        "linrm = LinearRegressionModel()\n",
        "linrm.w = np.array([[-2.0]])\n",
        "linrm.b = np.array([[-3.0]])\n",
        "\n",
        "loss = linrm.fit(dg.x, dg.t, 0.01, 200)\n",
        "\n",
        "tol = 1.e-15\n",
        "assert np.abs(loss[-1] - 2.5208907868478994) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GvojOR6PwAh"
      },
      "source": [
        "Plot loss versus iteration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t25is32WlB3"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(loss)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSpd6cH1P7s_"
      },
      "source": [
        "Plot the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S17gVyVJKAhu"
      },
      "source": [
        "xx = dg.modelx\n",
        "yy = linrm.predict(xx)\n",
        "dg.plot_dataset(estimation=(xx, yy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4HuhG7aMQyH"
      },
      "source": [
        "Test on 2D data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arl_5DMSMEWn"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[2.0, 2.0])\n",
        "dg.create_dataset(n=500, seed=17)\n",
        "\n",
        "linrm = LinearRegressionModel(2)\n",
        "linrm.w = np.array([[-2.0], [-2.0]])\n",
        "linrm.b = np.array([[-3.0]])\n",
        "\n",
        "loss = linrm.fit(dg.x, dg.t, 0.01, 100)\n",
        "\n",
        "tol = 1.e-15\n",
        "assert np.abs(loss[-1] - 2.660068435196912) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS3JnDn9RBwL"
      },
      "source": [
        "Plot loss versus iteration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPCycfK4_HqP"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(loss)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApalCKE4RWI7"
      },
      "source": [
        "Plot predictions versus targets (should distribute along $y=x$):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD1f1QXFRVFu"
      },
      "source": [
        "y = linrm.predict(dg.x)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(dg.t, y, 'o')\n",
        "plt.plot([dg.t.min(), dg.t.max()], [dg.t.min(), dg.t.max()], 'r-')\n",
        "plt.xlabel('real')\n",
        "plt.ylabel('predicted')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYaB_sTlRF-T"
      },
      "source": [
        "Plot the a 3D representation of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4N9Vmdqs7LE"
      },
      "source": [
        "xx, yy = np.meshgrid(dg.modelx, dg.modelx)\n",
        "zz = linrm.predict(np.concatenate((xx.ravel()[:, None], yy.ravel()[:, None]), axis=1)).reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.plot_surface(xx, yy, zz, cmap='viridis')\n",
        "ax.set_xlabel('$x_{1}$')\n",
        "ax.set_ylabel('$x_{2}$')\n",
        "ax.set_zlabel('$y$')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9bjZx00MZgv"
      },
      "source": [
        "### <font color=\"#CA3532\">Exercise 2. Logistic regression</font>\n",
        "\n",
        "In the file ``dlfbt_lab1.py``, complete the code of the class ``LogisticRegressionModel`` by filling the ``TO-DO`` block, so that it implements a Logistic Regression model. Note that this class is an extension of the class ``LinearRegressionModel``. You should overwrite the necessary methods only.\n",
        "\n",
        "You can run the following code to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dlfbt_lab1 import LogisticRegressionModel"
      ],
      "metadata": {
        "id": "NTgDy7RLlV6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cictnUnbxECN"
      },
      "source": [
        "**1. Test the ``predict`` method:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diuYAzUKxclN"
      },
      "source": [
        "Test on a single scalar value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGWA2IjTxOJ5"
      },
      "source": [
        "logrm = LogisticRegressionModel()\n",
        "logrm.w = np.array([[2.0]])\n",
        "logrm.b = np.array([[-2.0]])\n",
        "\n",
        "x = np.array([[2.0]])\n",
        "t = np.array([[1.0]])\n",
        "y = logrm.predict(x)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert y.shape == (1, 1)\n",
        "assert np.abs(y[0][0] - 0.88079708) < tol\n",
        "assert np.abs(logrm.get_loss(x, t) - 0.1269280110) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kelleFbUxgEl"
      },
      "source": [
        "Test on 1D data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPqSBeWFxOm-"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLogistic()\n",
        "dg.create_dataset(seed=17)\n",
        "\n",
        "logrm = LogisticRegressionModel()\n",
        "logrm.w = dg.a\n",
        "logrm.b = dg.b\n",
        "\n",
        "y = logrm.predict(dg.x)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert y.shape == (1000, 1)\n",
        "assert np.abs(logrm.get_loss(dg.x, dg.t) - 0.1796979922586585) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZOdzKCFxiCo"
      },
      "source": [
        "Test on multidimensional data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpfY8e59xN5v"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLogistic(a=[-5.0, 2.0, -3.0, 2.0])\n",
        "dg.create_dataset(n=500, seed=17)\n",
        "\n",
        "logrm = LogisticRegressionModel()\n",
        "logrm.w = dg.a\n",
        "logrm.b = dg.b\n",
        "\n",
        "y = logrm.predict(dg.x)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert y.shape == (500, 1)\n",
        "assert np.abs(logrm.get_loss(dg.x, dg.t) - 0.020315686967631474) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM5yxu3y86jc"
      },
      "source": [
        "**2. Test the ``fit`` method:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7SqFjyU9OEg"
      },
      "source": [
        "Test on 1D data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEAsdVaL852-"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLogistic1D()\n",
        "dg.create_dataset(seed=17)\n",
        "\n",
        "logrm = LogisticRegressionModel()\n",
        "logrm.w = np.array([[-2.0]])\n",
        "logrm.b = np.array([[-3.0]])\n",
        "\n",
        "loss = logrm.fit(dg.x, dg.t, 0.01, 15000)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert np.abs(logrm.get_loss(dg.x, dg.t) - 0.1960565561171435) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNCJZlYKdeJy"
      },
      "source": [
        "Plot loss versus iteration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRcPakfednaQ"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(loss)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKCxFuFydwN-"
      },
      "source": [
        "Plot the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNJoMXo-91z6"
      },
      "source": [
        "xx = dg.modelx\n",
        "yy = logrm.predict(xx)\n",
        "dg.plot_dataset(estimation=(xx, yy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zghgKLt99_7L"
      },
      "source": [
        "Test on 2D data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPEhxBe992Ux"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLogistic(a=[2.0, 2.0], b=-20.0)\n",
        "dg.create_dataset(n=500, seed=17)\n",
        "\n",
        "logrm = LogisticRegressionModel(2)\n",
        "logrm.w = np.array([[-2.0], [-2.0]])\n",
        "logrm.b = np.array([[-3.0]])\n",
        "\n",
        "loss = logrm.fit(dg.x, dg.t, 0.01, 50000)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert np.abs(loss[-1] - 0.17895705991377248) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVqYaa_veyp0"
      },
      "source": [
        "Plot loss versus iteration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFaqsgNIeAwv"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(loss)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGJI_fQ_e-Bu"
      },
      "source": [
        "Plot the a 3D representation of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr56S_R2d_6m"
      },
      "source": [
        "xx, yy = np.meshgrid(dg.modelx, dg.modelx)\n",
        "zz = logrm.predict(np.concatenate((xx.ravel()[:, None], yy.ravel()[:, None]), axis=1)).reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.plot_surface(xx, yy, zz, cmap='RdYlBu')\n",
        "ax.set_xlabel('$x_{1}$')\n",
        "ax.set_ylabel('$x_{2}$')\n",
        "ax.set_zlabel('$y$')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xer4SGVjcJL"
      },
      "source": [
        "Contour plot with data points superimposed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTidSPJ8DVEL"
      },
      "source": [
        "ix0 = (dg.t == 0).ravel()\n",
        "ix1 = (dg.t == 1).ravel()\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.contourf(xx, yy, zz, 40, cmap='RdYlBu', alpha=0.8)\n",
        "plt.plot(dg.x[ix0, 0], dg.x[ix0, 1], 'or', label=\"target = 0\")\n",
        "plt.plot(dg.x[ix1, 0], dg.x[ix1, 1], 'ob', label=\"target = 1\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.axis([0, 10, 0, 10])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FthTQXQzFf_l"
      },
      "source": [
        "### <font color=\"#CA3532\">Introduction to TensorFlow</font>\n",
        "\n",
        "Read the following introduction to TensorFlow before proceeding with the next exercises.\n",
        "\n",
        "We will use TensorFlow 2. The following cell imports the library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaHYUdYXEe1u"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pNMAWpEnm9i"
      },
      "source": [
        "### <font color=\"#CA3532\">Basic concepts</font>\n",
        "\n",
        "A **tensor** is a data array with an arbitrary number of dimensions. This number of dimensions is called the tensor *rank*. Hence:\n",
        "\n",
        "- A scalar is a tensor of rank 0.\n",
        "- A vector is a tensor of rank 1.\n",
        "- A matrix is a tensor of rank 2.\n",
        "\n",
        "Let us see some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8e5Kh1DniUu"
      },
      "source": [
        "# A scalar is a tensor of rank 0:\n",
        "t0 = tf.constant(3.)\n",
        "print(t0)\n",
        "print(t0.numpy())\n",
        "print(tf.shape(t0).numpy())\n",
        "print(tf.rank(t0).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ-ISVqAnrNY"
      },
      "source": [
        "# A vector is a tensor of rank 1:\n",
        "t1 = tf.constant([1., 2., 3.])\n",
        "print(t1)\n",
        "print(t1.numpy())\n",
        "print(tf.shape(t1).numpy())\n",
        "print(tf.rank(t1).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXyhRqefntyZ"
      },
      "source": [
        "# A matrix is a tensor of rank 2:\n",
        "t2 = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "print(t2)\n",
        "print(t2.numpy())\n",
        "print(tf.shape(t2).numpy())\n",
        "print(tf.rank(t2).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Yn4H8wnwap"
      },
      "source": [
        "# We may consider tensors of rank 3 and higher:\n",
        "t3 = tf.constant([[[1., 2., 3.]], [[7., 8., 9.]]])\n",
        "print(t3)\n",
        "print(t3.numpy())\n",
        "print(tf.shape(t3).numpy())\n",
        "print(tf.rank(t3).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avQvEEuzo5oD"
      },
      "source": [
        "### <font color=\"#CA3532\">The computational graph</font>\n",
        "\n",
        "Whenever we perform an operation on tensors, TensorFlow builds a *computational graph*:\n",
        "\n",
        "- All constant and variable tensors are input nodes in the computational graph.\n",
        "\n",
        "- All operators are graph nodes that perform an operation on their tensor inputs and generate a new tensor as output.\n",
        "\n",
        "Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qg37Kyfn0Q4"
      },
      "source": [
        "# Two constant tensors as input:\n",
        "a = tf.constant(15)\n",
        "b = tf.constant(61)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "# Tensor addition (both tf.add and the + operator have the same meaning):\n",
        "c1 = tf.add(a,b)\n",
        "c2 = a + b\n",
        "print(c1)\n",
        "print(c2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1mRxcflqQHD"
      },
      "source": [
        "### <font color=\"#CA3532\">The backward pass</font>\n",
        "\n",
        "The computational graph is usually used to compute the output of the function the graph represents on given inputs. This is called the **forward pass**.\n",
        "\n",
        "But it can also be used to compute the gradients of the output with respect to any of the (variable) inputs in a **backward pass** that traverses the graph in the reverse direction applying the chain rule of the derivative at each node.\n",
        "\n",
        "To perform a backward pass in a computational graph the following two things are necessary:\n",
        "\n",
        "1. The graph must have at least one variable input (a variable is a tensor created with ``tf.Variable``).\n",
        "\n",
        "2. The graph must be created in the context of a *gradient tape*. During the forward pass the gradient tape records all the necessary information to compute the gradients.\n",
        "\n",
        "As an example, we will create a computational graph for the operation $3x^{2}$, where $x$ is a variable initialized to $1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUWMmFuBqKsq"
      },
      "source": [
        "# Constant and variable tensors:\n",
        "a = tf.constant(3.0)\n",
        "x = tf.Variable(1.0)\n",
        "\n",
        "# Define the graph wihtin a gradient tape:\n",
        "with tf.GradientTape() as tape:\n",
        "  y = a*x*x\n",
        "\n",
        "# Print y:\n",
        "print(\"y =\", y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q08aVodNqXT6"
      },
      "source": [
        "Now we may use the gradient tape to compute the gradient of $y$ with respect to $x$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peTIlmKLqUJ-"
      },
      "source": [
        "dy_dx = tape.gradient(y, x)\n",
        "print(\"dy_dx =\", dy_dx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hXLePkMqcbr"
      },
      "source": [
        "It is useful to place all the code in a function so that it is more easily reused:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGsCbg59qZ5O"
      },
      "source": [
        "def ax2(a, x):\n",
        "  x = tf.Variable(x)\n",
        "\n",
        "  # Define the graph wihtin a gradient tape:\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = a*x*x # Note that a is interpreted as a contant tensor\n",
        "\n",
        "  # Gradient computation:\n",
        "  dy_dx = tape.gradient(y, x)\n",
        "\n",
        "  return y, dy_dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9mIw7mMqicD"
      },
      "source": [
        "Let us do some tests:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xrycn59qgKf"
      },
      "source": [
        "y, dy_dx = ax2(3.0, 1.0)\n",
        "print(\"y =\", y)\n",
        "print(\"dy_dx =\", dy_dx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyyTnOc3qlf-"
      },
      "source": [
        "y, dy_dx = ax2(3.0, 3.0)\n",
        "print(\"y =\", y)\n",
        "print(\"dy_dx =\", dy_dx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3X55ZZzqq1R"
      },
      "source": [
        "You may check that both the value of the function and its derivative are correct."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"#CA3532\">Exercise 3. Use TensorFlow to evaluate the derivative of a function</font>\n",
        "\n",
        "In the file ``dlfbt_lab1.py``, complete the code of the function ``BasicTF.differentiate(f, x)`` by filling the ``TO-DO`` block, so that it returns the derivative of the function ``f`` evaluated on ``x``.\n",
        "\n",
        "You can run the following code to test your implementation."
      ],
      "metadata": {
        "id": "tl_xv80PZAk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dlfbt_lab1 import BasicTF"
      ],
      "metadata": {
        "id": "DNXJ-qS6Y_3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf_52fxbuLCx"
      },
      "source": [
        "Use the function to plot a graph of of the $cosine$ function and its derivative:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x : tf.cos(x)\n",
        "\n",
        "x = np.arange(0, 2*np.pi, 0.01)\n",
        "y, dy_dx = f(x), BasicTF.differentiate(f, x)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(x, y, label=\"y\")\n",
        "plt.plot(x, dy_dx, label=\"dy_dx\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.xlabel(\"x\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nB8hffZvaic1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the function to plot a graph of of the $sigmoid$ function and its derivative:"
      ],
      "metadata": {
        "id": "oVBac8iLblVb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywFg9KG5wJjp"
      },
      "source": [
        "f = lambda x : tf.sigmoid(x)\n",
        "\n",
        "x = np.arange(-10., 10., 0.01)\n",
        "y, dy_dx = f(x), BasicTF.differentiate(f, x)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(x, y)\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(x, dy_dx)\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"dy_dx\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRNMEj-RxPsZ"
      },
      "source": [
        "### <font color=\"#CA3532\">Exercise 4. Gradient descent to find the minimum of a function</font>\n",
        "\n",
        "We can use the automatic differentiation of TensorFlow to minimize a function using the gradient descent method.\n",
        "\n",
        "In the file ``dlfbt_lab1.py``, complete the code of the function ``BasicTF.gradient_descent(f, x0, learning_rate, niters)`` that finds a minimum of a function using gradient descent. The function receives the following arguments:\n",
        "\n",
        "- ``f``: the function to minimize\n",
        "- ``x0``: the initial value\n",
        "- ``learning_rate``: the learning rate\n",
        "- ``niters``: the number of iterations\n",
        "\n",
        "The function returns a Numpy array with all the values of $x$.\n",
        "\n",
        "You can run the following code to test your implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the minimum of the function $f(x) = (x-4)^{2}$:"
      ],
      "metadata": {
        "id": "YDNHDHz6f2YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x : (x - 4)**2\n",
        "x_history = BasicTF.gradient_descent(f, 0.0, 1.e-2, 500)\n",
        "\n",
        "# Plot x versus iteration:\n",
        "plt.plot(x_history)\n",
        "plt.plot([0, 500], [4, 4])\n",
        "plt.grid(True)\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uyBYmIc5pNn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find a local minimum of the function $f(x) = \\cos x$:"
      ],
      "metadata": {
        "id": "imCjdKC3gGiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x : tf.cos(x)\n",
        "x_history = BasicTF.gradient_descent(f, 1.0, 1.e-2, 1000)\n",
        "\n",
        "# Plot x versus iteration:\n",
        "plt.plot(x_history)\n",
        "plt.plot([0, 1000], [np.pi, np.pi])\n",
        "plt.grid(True)\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9xNPxipgHt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQIl9AP60Ooj"
      },
      "source": [
        "### <font color=\"#CA3532\">Exercise 5. Linear regression</font>\n",
        "\n",
        "Complete the code of the class ``LinearRegressionModel_TF`` by filling all the ``TO-DO`` blocks, so that it implements a Linear Regression model using TensorFlow. Note that the gradients need not be explicitly computed and must be obtained from the *gradient tape*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UXFb6qyxWyt"
      },
      "source": [
        "from dlfbt_lab1 import LinearRegressionModel_TF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fUPKfGwCJ2X"
      },
      "source": [
        "Run the following tests to check your implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpkCLMOZCQa7"
      },
      "source": [
        "Test the ``predict`` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3apq6eF98rnT"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[-5.0, 2.0, -3.0, 2.0])\n",
        "dg.create_dataset(noise=0.0, n=500)\n",
        "\n",
        "linrm = LinearRegressionModel_TF()\n",
        "linrm.w = tf.Variable(dg.a)\n",
        "linrm.b = tf.Variable(dg.b)\n",
        "\n",
        "y = linrm.predict(dg.x)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert y.shape == (500, 1)\n",
        "assert np.abs(y - dg.t).max() < tol\n",
        "assert linrm.get_loss(dg.x, dg.t) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CJ0hkNJCZms"
      },
      "source": [
        "Test the ``compute_gradients`` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_yMrXh89D1P"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[-5.0, 2.0, -3.0, 2.0])\n",
        "dg.create_dataset(n=50000, seed=17)\n",
        "\n",
        "linrm = LinearRegressionModel_TF()\n",
        "linrm.w = tf.Variable(dg.a)\n",
        "linrm.b = tf.Variable(dg.b)\n",
        "\n",
        "db, dw = linrm.compute_gradients(dg.x, dg.t)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert db.shape == (1, 1)\n",
        "assert np.abs(db[0][0] + 0.00704326) < tol\n",
        "assert dw.shape == (4, 1)\n",
        "assert np.abs(dw[0][0] + 0.05353578) < tol\n",
        "assert np.abs(dw[1][0] + 0.03276935) < tol\n",
        "assert np.abs(dw[2][0] + 0.00337341) < tol\n",
        "assert np.abs(dw[3][0] + 0.03293776) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79dKAGvCg4g"
      },
      "source": [
        "Test the ``fit`` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJFfMn5r-Gqz"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[2.0, 2.0])\n",
        "dg.create_dataset(n=500, seed=17)\n",
        "\n",
        "linrm = LinearRegressionModel_TF(2)\n",
        "linrm.w = tf.Variable([[-2.0], [-2.0]], dtype=tf.dtypes.float64)\n",
        "linrm.b = tf.Variable([[-3.0]], dtype=tf.dtypes.float64)\n",
        "\n",
        "loss = linrm.fit(dg.x, dg.t, 0.01, 100)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert np.abs(loss[-1] - 2.660068435196912) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjLGuzjACodt"
      },
      "source": [
        "Compare the two linear regression implementations (with Numpy and with TensorFlow):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuNrXfw6CurD"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[-5.0, 2.0, -3.0, 2.0])\n",
        "dg.create_dataset(n=50000, seed=17)\n",
        "\n",
        "# Numpy implementation:\n",
        "linrm_np = LinearRegressionModel(4)\n",
        "\n",
        "# TensorFlow implementation:\n",
        "linrm_tf = LinearRegressionModel_TF()\n",
        "linrm_tf.w = tf.Variable(linrm_np.w.copy())\n",
        "linrm_tf.b = tf.Variable(linrm_np.b.copy())\n",
        "\n",
        "# Fit both models to data:\n",
        "loss_np = linrm_np.fit(dg.x, dg.t, 0.01, 100)\n",
        "loss_tf = linrm_tf.fit(dg.x, dg.t, 0.01, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GkPSQRDEUPn"
      },
      "source": [
        "The two curves should overlap:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vzHGDYCDNET"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(loss_np, label=\"Numpy impl\")\n",
        "plt.plot(loss_tf, label=\"TF impl\")\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PCq20qYRO9S"
      },
      "source": [
        "### <font color=\"#CA3532\">Exercise 6. Implementation of a neural network with Numpy</font>\n",
        "\n",
        "Complete the code of the class ``NeuralNetwork`` by filling all the ``TO-DO`` blocks, so that it implements a Neural Network model using Numpy only. You must explicitly calculate the gradients.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrfdDKYhSg8L"
      },
      "source": [
        "from dlfbt_lab1 import NeuralNetwork"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VQbyI1ifxDq"
      },
      "source": [
        "You may run the following tests to check your implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ7QrFoLlher"
      },
      "source": [
        "**1. Test the ``predict``method:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdclH-I0WRoD"
      },
      "source": [
        "np.random.seed(17)\n",
        "x = np.random.randn(3, 20)\n",
        "\n",
        "net = NeuralNetwork([(3, 'input'), (10, 'sigmoid'), (1, 'sigmoid')])\n",
        "z, y = net.predict(x)\n",
        "\n",
        "assert z[0].shape == (10, 20)\n",
        "assert y[0].shape == (10, 20)\n",
        "assert z[1].shape == (1, 20)\n",
        "assert y[1].shape == (1, 20)\n",
        "\n",
        "# Array values, should match those on the test file:\n",
        "with open('DLFBT-LAB/data/test_nn_numpy_predict.pickle', 'rb') as handle:\n",
        "    [zexp, yexp] = pickle.load(handle)\n",
        "\n",
        "tol = 1.e-8\n",
        "for zp, yp, ze, ye in zip(z, y, zexp, yexp):\n",
        "    assert np.max(np.abs(zp - ze)) < tol\n",
        "    assert np.max(np.abs(yp - ye)) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3UohMcslklG"
      },
      "source": [
        "**2. Test the ``compute_gradients`` method:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnK3mwBWRjD"
      },
      "source": [
        "np.random.seed(17)\n",
        "x = np.random.randn(3, 20)\n",
        "t = np.random.randn(1, 20)\n",
        "\n",
        "net = NeuralNetwork([(3, 'input'), (10, 'sigmoid'), (1, 'linear')])\n",
        "dW, db = net.compute_gradients(x, t)\n",
        "\n",
        "assert dW[0].shape == (10, 3)\n",
        "assert db[0].shape == (10, 1)\n",
        "assert dW[1].shape == (1, 10)\n",
        "assert db[1].shape == (1, 1)\n",
        "\n",
        "# Array values, should match those on the test file:\n",
        "with open('DLFBT-LAB/data/test_nn_numpy_compute_gradients.pickle', 'rb') as handle:\n",
        "    [dWexp, dbexp] = pickle.load(handle)\n",
        "\n",
        "tol = 1.e-8\n",
        "for dwp, dbp, dwe, dbe in zip(dW, db, dWexp, dbexp):\n",
        "    assert np.max(np.abs(dwp - dwe)) < tol\n",
        "    assert np.max(np.abs(dbp - dbe)) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rgwjyXdlo1b"
      },
      "source": [
        "**3. Test the ``fit`` method on a regression problem:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHddBmSMf_kr"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[2.0, 2.0])\n",
        "dg.create_dataset(n=1000, seed=17, noise=1.0)\n",
        "x = dg.x.transpose()\n",
        "t = dg.t.transpose()\n",
        "\n",
        "np.random.seed(23)\n",
        "net = NeuralNetwork([(2, 'input'), (1, 'linear')])\n",
        "loss = net.fit(x, t, 0.01, 100, 1000, NeuralNetwork.mse_loss)\n",
        "z, y = net.predict(x)\n",
        "\n",
        "assert z[0].shape == (1, 1000)\n",
        "assert y[0].shape == (1, 1000)\n",
        "\n",
        "# Array values, should match those on the test file:\n",
        "with open('DLFBT-LAB/data/test_nn_numpy_fit.pickle', 'rb') as handle:\n",
        "    [zexp, yexp] = pickle.load(handle)\n",
        "\n",
        "tol = 1.e-8\n",
        "for zp, yp, ze, ye in zip(z, y, zexp, yexp):\n",
        "    assert np.max(np.abs(zp - ze)) < tol\n",
        "    assert np.max(np.abs(yp - ye)) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR3NbgV7lpyP"
      },
      "source": [
        "Plot loss versus epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53tsxB7cnaND"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kK_fKGXlwM6"
      },
      "source": [
        "Plot predictions versus targets (should distribute along $y=x$):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3T9_b4Plx8p"
      },
      "source": [
        "_, y = net.predict(x)\n",
        "y = y[-1].transpose()\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(dg.t, y, 'o')\n",
        "plt.plot([dg.t.min(), dg.t.max()], [dg.t.min(), dg.t.max()], 'r-')\n",
        "plt.xlabel('real')\n",
        "plt.ylabel('predicted')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghPmPIqaRQtE"
      },
      "source": [
        "### <font color=\"#CA3532\">Exercise 7. Implementation of a neural network with TensorFlow</font>\n",
        "\n",
        "Complete the code of the class ``NeuralNetwork_TF`` by filling all the ``TO-DO`` blocks, so that it implements a Neural Network model using TensorFlow. Now the gradients should be obtained from the gradient tape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGrQ7ikpRP0_"
      },
      "source": [
        "from dlfbt_lab1 import NeuralNetwork_TF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4L50bK9pQxi"
      },
      "source": [
        "You may run the following tests to check your implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu6oBx622I-9"
      },
      "source": [
        "**1. Test the ``predict``method:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sYISRu72Rk8"
      },
      "source": [
        "np.random.seed(17)\n",
        "tf.random.set_seed(17)\n",
        "x = np.random.randn(3, 20)\n",
        "\n",
        "net = NeuralNetwork_TF([(3, None), (10, tf.sigmoid), (1, tf.sigmoid)])\n",
        "y = net.predict(x)\n",
        "\n",
        "assert y.shape == (1, 20)\n",
        "\n",
        "# Array values, should match those on the test file:\n",
        "with open('DLFBT-LAB/data/test_nn_tf_predict.pickle', 'rb') as handle:\n",
        "    yexp = pickle.load(handle)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert np.max(np.abs(y - yexp)) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACJyRIwR66TO"
      },
      "source": [
        "**2. Test the ``compute_gradients`` method:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIRwsjNG2m9s"
      },
      "source": [
        "np.random.seed(17)\n",
        "tf.random.set_seed(17)\n",
        "x = np.random.randn(3, 20)\n",
        "t = np.random.randn(1, 20)\n",
        "\n",
        "net = NeuralNetwork_TF([(3, None), (10, tf.sigmoid), (1, tf.identity)])\n",
        "db, dW = net.compute_gradients(x, t, NeuralNetwork_TF.mse_loss)\n",
        "\n",
        "assert dW[0].shape == (10, 3)\n",
        "assert db[0].shape == (10, 1)\n",
        "assert dW[1].shape == (1, 10)\n",
        "assert db[1].shape == (1, 1)\n",
        "\n",
        "# Array values, should match those on the test file:\n",
        "with open('DLFBT-LAB/data/test_nn_tf_compute_gradients.pickle', 'rb') as handle:\n",
        "    [dWexp, dbexp] = pickle.load(handle)\n",
        "\n",
        "tol = 1.e-8\n",
        "for dwp, dbp, dwe, dbe in zip(dW, db, dWexp, dbexp):\n",
        "    assert np.max(np.abs(dwp - dwe)) < tol\n",
        "    assert np.max(np.abs(dbp - dbe)) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oee-XgdK-Fo6"
      },
      "source": [
        "**3. Test the ``fit`` method on a regression problem:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti0dAAfl-Elh"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[2.0, 2.0])\n",
        "dg.create_dataset(n=1000, seed=17, noise=1.0)\n",
        "x = dg.x.transpose()\n",
        "t = dg.t.transpose()\n",
        "\n",
        "np.random.seed(23)\n",
        "tf.random.set_seed(23)\n",
        "net = NeuralNetwork_TF([(2, None), (1, tf.identity)])\n",
        "loss = net.fit(x, t, 0.01, 100, 1000, NeuralNetwork_TF.mse_loss)\n",
        "y = net.predict(x)\n",
        "\n",
        "# Array shape, should be (1, 1000):\n",
        "assert y.shape == (1, 1000)\n",
        "\n",
        "# Array values, should match those on the test file, so that all prints are 0\n",
        "# or close to 0:\n",
        "with open('DLFBT-LAB/data/test_nn_tf_fit.pickle', 'rb') as handle:\n",
        "    yexp = pickle.load(handle)\n",
        "\n",
        "tol = 1.e-8\n",
        "assert np.max(np.abs(y - yexp)) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-4Vrvy7Z-D"
      },
      "source": [
        "Plot loss versus epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghXjsdAH7_Sl"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKe5tlRgAkyG"
      },
      "source": [
        "Plot predictions versus targets (should distribute along $y=x$):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADa2-5oz8DHV"
      },
      "source": [
        "y = net.predict(x).numpy().transpose()\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(dg.t, y, 'o')\n",
        "plt.plot([dg.t.min(), dg.t.max()], [dg.t.min(), dg.t.max()], 'r-')\n",
        "plt.xlabel('real')\n",
        "plt.ylabel('predicted')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5wij5qACu1H"
      },
      "source": [
        "### <font color=\"#CA3532\">Comparison between the two implementations</font>\n",
        "\n",
        "The following are additional tests to check that the two implementations (with Numpy and with TensorFlow) provide the same results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlkE7MjsXlUq"
      },
      "source": [
        "``predict``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieFbmL0X8u0n"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLinear(a=[-5.0, 2.0, -3.0, 2.0])\n",
        "dg.create_dataset(n=50000, seed=17)\n",
        "x = dg.x.transpose()\n",
        "t = dg.t.transpose()\n",
        "\n",
        "# Numpy implementation:\n",
        "r_np = NeuralNetwork([(4, 'input'), (10, 'sigmoid'), (1, 'linear')])\n",
        "\n",
        "# TensorFlow implementation:\n",
        "r_tf = NeuralNetwork_TF([(4, None), (10, tf.sigmoid), (1, tf.identity)])\n",
        "\n",
        "# Copy weights and biases from numpy to tf:\n",
        "r_tf.b = [tf.Variable(b.copy()) for b in r_np.b]\n",
        "r_tf.W = [tf.Variable(w.copy()) for w in r_np.W]\n",
        "\n",
        "# Run predict for both models:\n",
        "_, ynp = r_np.predict(x)\n",
        "ynp = ynp[-1]\n",
        "ytf = r_tf.predict(x)\n",
        "\n",
        "# Plot should be diagonal:\n",
        "plt.plot(ynp[0], ytf[0], '.')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Difference should be close to 0:\n",
        "tol = 1.e-8\n",
        "assert np.max(np.abs(ynp-ytf)) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4WY9iBLXsKI"
      },
      "source": [
        "``compute_gradients``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aHMlu-pIAhG"
      },
      "source": [
        "# Compute the gradients with both models:\n",
        "dW_np, db_np = r_np.compute_gradients(x, t)\n",
        "db_tf, dW_tf = r_tf.compute_gradients(x, t, NeuralNetwork_TF.mse_loss)\n",
        "\n",
        "# Difference should be close to 0:\n",
        "tol = 1.e-8\n",
        "assert np.max(np.abs(db_np[0] - db_tf[0].numpy())) < tol\n",
        "assert np.max(np.abs(db_np[1] - db_tf[1].numpy())) < tol\n",
        "assert np.max(np.abs(dW_np[0] - dW_tf[0].numpy())) < tol\n",
        "assert np.max(np.abs(dW_np[1] - dW_tf[1].numpy())) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxjb9zCTX743"
      },
      "source": [
        "``fit``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHJ3xp0nIA28"
      },
      "source": [
        "# Run fit on both models:\n",
        "np.random.seed(seed=233)\n",
        "loss_np = r_np.fit(x, t, 0.01, 100, 5000, NeuralNetwork.mse_loss)\n",
        "np.random.seed(seed=233)\n",
        "loss_tf = r_tf.fit(x, t, 0.01, 100, 5000, NeuralNetwork_TF.mse_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaHP-EVYC-dm"
      },
      "source": [
        "# The two curves should overlap:\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(loss_np, label=\"Numpy impl\")\n",
        "plt.plot(loss_tf, label=\"TF impl\")\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvaZdQESDEGT"
      },
      "source": [
        "# Predict after training:\n",
        "_, ynp = r_np.predict(x)\n",
        "ynp = ynp[-1]\n",
        "ytf = r_tf.predict(x)\n",
        "\n",
        "# Plot should be diagonal:\n",
        "plt.plot(ynp[0], ytf[0], '.')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Difference should be close to 0:\n",
        "tol = 1.e-8\n",
        "assert np.max(np.abs(ynp-ytf)) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-rNNAm8Dflc"
      },
      "source": [
        "Same as before with a classification problem:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR_vDntXAOGU"
      },
      "source": [
        "dg = dlfbt.DataGeneratorLogistic(a=[-5.0, 2.0, -3.0, 2.0])\n",
        "dg.create_dataset(n=50000, seed=17)\n",
        "x = dg.x.transpose()\n",
        "t = dg.t.transpose()\n",
        "\n",
        "# Numpy implementation:\n",
        "r_np = NeuralNetwork([(4, 'input'), (10, 'sigmoid'), (1, 'sigmoid')])\n",
        "\n",
        "# TensorFlow implementation:\n",
        "r_tf = NeuralNetwork_TF([(4, None), (10, tf.sigmoid), (1, tf.sigmoid)])\n",
        "\n",
        "# Copy weights and biases from numpy to tf:\n",
        "r_tf.b = [tf.Variable(b.copy()) for b in r_np.b]\n",
        "r_tf.W = [tf.Variable(w.copy()) for w in r_np.W]\n",
        "\n",
        "# Run predict on both models:\n",
        "_, ynp = r_np.predict(x)\n",
        "ynp = ynp[-1]\n",
        "ytf = r_tf.predict(x)\n",
        "\n",
        "# Plot should be diagonal:\n",
        "plt.plot(ynp[0], ytf[0], '.')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Difference should be close to 0:\n",
        "tol = 1.e-8\n",
        "assert np.max(np.abs(ynp-ytf)) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwvxFWPMD5Ew"
      },
      "source": [
        "# Compute the gradients with both models:\n",
        "dW_np, db_np = r_np.compute_gradients(x, t)\n",
        "db_tf, dW_tf = r_tf.compute_gradients(x, t, NeuralNetwork_TF.cross_entropy_loss)\n",
        "\n",
        "# Difference should be close to 0:\n",
        "tol = 1.e-8\n",
        "assert np.max(np.abs(db_np[0] - db_tf[0].numpy())) < tol\n",
        "assert np.max(np.abs(db_np[1] - db_tf[1].numpy())) < tol\n",
        "assert np.max(np.abs(dW_np[0] - dW_tf[0].numpy())) < tol\n",
        "assert np.max(np.abs(dW_np[1] - dW_tf[1].numpy())) < tol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yvnk8HlD5h8"
      },
      "source": [
        "# Run fit on both models:\n",
        "np.random.seed(seed=233)\n",
        "loss_np = r_np.fit(x, t, 0.01, 100, 5000, NeuralNetwork.cross_entropy_loss)\n",
        "np.random.seed(seed=233)\n",
        "loss_tf = r_tf.fit(x, t, 0.01, 100, 5000, NeuralNetwork_TF.cross_entropy_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CXCX1H1D5WA"
      },
      "source": [
        "# The two curves should overlap:\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(loss_np, label=\"Numpy impl\")\n",
        "plt.plot(loss_tf, label=\"TF impl\")\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXEp13_REkFJ"
      },
      "source": [
        "# Run predict on trained models:\n",
        "_, ynp = r_np.predict(x)\n",
        "ynp = ynp[-1]\n",
        "ytf = r_tf.predict(x)\n",
        "\n",
        "# Plot should be diagonal:\n",
        "plt.plot(ynp[0], ytf[0], '.')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Difference should be close to 0:\n",
        "tol = 1.e-8\n",
        "assert np.max(np.abs(ynp-ytf)) < tol"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}